{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trade data including exports and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#import requests\n",
    "import zipfile as zf \n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Comment 3 cells below after data is downloaded and unzipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nurl=\"http://fenixservices.fao.org/faostat/static/bulkdownloads/Trade_DetailedTradeMatrix_E_All_Data_(Normalized).zip\"\\nr = requests.get(url)\\n#300 MB dowload, slow...\\n\\nfilename = Path(\\'Trade_DetailedTradeMatrix_E_All_Data_(Normalized).zip\\')\\nfilename.write_bytes(r.content) \\n\\nfiles = zf.ZipFile(\\'Trade_DetailedTradeMatrix_E_All_Data_(Normalized).zip\\', \\'r\\')\\nfiles.extractall(\\'\\')\\nfiles.close()\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "url=\"http://fenixservices.fao.org/faostat/static/bulkdownloads/Trade_DetailedTradeMatrix_E_All_Data_(Normalized).zip\"\n",
    "r = requests.get(url)\n",
    "#300 MB dowload, slow...\n",
    "\n",
    "filename = Path('Trade_DetailedTradeMatrix_E_All_Data_(Normalized).zip')\n",
    "filename.write_bytes(r.content) \n",
    "\n",
    "files = zf.ZipFile('Trade_DetailedTradeMatrix_E_All_Data_(Normalized).zip', 'r')\n",
    "files.extractall('')\n",
    "files.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can just start here if you download the linked file and unzip it in the Data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pd_main \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../../Data/raw_trade/Trade_DetailedTradeMatrix_E_All_Data_(Normalized).csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlatin1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/trade/lib/python3.12/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/trade/lib/python3.12/site-packages/pandas/io/parsers/readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/trade/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1741\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m     (\n\u001b[1;32m   1745\u001b[0m         index,\n\u001b[1;32m   1746\u001b[0m         columns,\n\u001b[1;32m   1747\u001b[0m         col_dict,\n\u001b[0;32m-> 1748\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1749\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/trade/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:843\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:904\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2050\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<frozen codecs>:281\u001b[0m, in \u001b[0;36mgetstate\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pd_main = pd.read_csv('../../Data/raw_trade/Trade_DetailedTradeMatrix_E_All_Data_(Normalized).csv', sep=',', encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd_main[\"Year Code\"].unique())\n",
    "print(pd_main[\"Reporter Countries\"].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OK about flags raised in some data points after reading this http://www.fao.org/3/i3664e/i3664e.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete rows with same origin and destination, they relate to in-country free trade zones because \"Special trade it also includes imports into and exports from customs warehouses sand free zones or ports. In Special trade goods are recorded when cleared through customs for shome use or for export. Special imports include goods for domestic consumption and withdrawals from bonded warehouses or free zones for purposes of domestic consumption.\n",
    "\n",
    "Special exports comprise exports of goods wholly or partly produced or manufactured in the country, together with exports of \"nationalized\" goods;\" \n",
    "see http://fenixservices.fao.org/faostat/static/documents/T/T_e.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_main.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "same_country = pd_main.loc[(pd_main['Reporter Countries'] == pd_main['Partner Countries'])]\n",
    "same_country.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('National-trade instances:',len(same_country.index))\n",
    "print('International-trade instances:',len(pd_main.index) - len(same_country.index))\n",
    "\n",
    "pd_impexp = pd_main.drop(pd_main[(pd_main['Reporter Countries'] == pd_main['Partner Countries'])].index)\n",
    "\n",
    "print(len(pd_main.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Keep useful columns: \n",
    "Remove some unused columns: Flag column and Year Flag column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_cols = [\"Reporter Country Code\",\"Reporter Countries\",\"Partner Country Code\",\"Partner Countries\",\"Item\",\"Element\",\"Year\",\"Unit\",\"Value\"]\n",
    "pd_usecol = pd_impexp[useful_cols] \n",
    "print('Data after removal: ',len(pd_impexp.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd_usecol.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Understand unit and Element\n",
    "**Unit:** can be specified in many different categories, we want to standardise it for the whole dataset so the values can be compared. \\\n",
    "**Element:** \"Import Value / Quantities\" or \"Export Value / Quantities\". \n",
    "\n",
    "Import Quantity / Export Quantity-> tonnes \\\n",
    "Import Value / Export Value -> money ($)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd_usecol[\"Unit\"].unique())\n",
    "\n",
    "print(pd_usecol[\"Element\"].unique())\n",
    "pd_usecol.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check if element theory holds: \n",
    "\n",
    "imp_exp = 'Import' # 'Export' or 'Import'\n",
    "quantity_tonnes = pd_usecol[(pd_usecol[\"Unit\"] == 'tonnes') & (pd_usecol[\"Element\"] == imp_exp + ' Quantity')] \n",
    "print( imp_exp + ', tonnes, quantity:',len(quantity_tonnes.index))\n",
    "\n",
    "quantity_dollars = pd_usecol[(pd_usecol[\"Unit\"] == '1000 US$') & (pd_usecol[\"Element\"] == imp_exp + ' Quantity')] \n",
    "print( imp_exp + ', 1000US$, quantity:',len(quantity_dollars.index))\n",
    "\n",
    "value_dollars = pd_usecol[(pd_usecol[\"Unit\"] == '1000 US$') & (pd_usecol[\"Element\"] == imp_exp + ' Value')] \n",
    "print( imp_exp + ', 1000US$, value:',len(value_dollars.index))\n",
    "\n",
    "value_tonnes_exp= pd_usecol[(pd_usecol[\"Unit\"] == 'tonnes') & (pd_usecol[\"Element\"] == imp_exp + ' Value')] \n",
    "print( imp_exp + ', tonnes, value:',len(value_tonnes_exp.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the assessment we can Homogenise the Text from Quantity / Value to only indicate \"Import/Export\" (Takes 2 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd_usecol.loc[:,'Element']= pd_usecol.Element.str.split().str[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_usecol.loc[:,'Element']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Units: Keep only tonnes and USD \n",
    "Remove animals counted in units that have no clear translation to weight in tonnes (I tried, but there is no sensible source that provides homogeneous values across world regions). As shown below, it is less than 1% of the data.\n",
    "\n",
    "**Unit values**\n",
    "\n",
    "No= \"Bees\"\\\n",
    "1000 Head= Animals (chicken, birds, ducks, rabbits... geese)\\\n",
    "Head= Animals (goat, cattle, rodents, asses, camel, mule, buffalo)\\\n",
    "tonnes = other products\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "print(pd_usecol[\"Unit\"].unique())\n",
    "# test\n",
    "test = pd_usecol[(pd_usecol[\"Unit\"] == \"No\") ]\n",
    "#'1000 Head' 'Head' 'No'\n",
    "print(test[\"Item\"].unique())\n",
    "\n",
    "# Remove animal data: only keep data with unit: 'tonnes'or '1000 US$'.\n",
    "pd_usedata = pd_usecol[(pd_usecol[\"Unit\"] == 'tonnes') | (pd_usecol[\"Unit\"] == '1000 US$')] \n",
    "print(pd_usedata[\"Unit\"].unique())\n",
    "print('Samples in Tonnes or $:',pd_usecol.shape[0])\n",
    "print('Percentage of overlapping data',100*pd_usedata.shape[0]/pd_usecol.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Remove Redundant Items\n",
    "FAOSTAT responded via email that \"Rice, paddy (rice milled equivalent)\" is a summation of other included rice trades, and the only aggregated value that would lead to double counting, thus we eliminate it to keep greater detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_usedata = pd_usedata[(pd_usedata[\"Item\"] != 'Rice, paddy (rice milled equivalent)') ] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quality check\n",
    "\n",
    "Below a satisfactory quality check based on the web interface of faostat: http://www.fao.org/faostat/en/#rankings/countries_by_commodity_imports\n",
    "\n",
    "##### Top 20 Country Importers, Import Quantity of Wheat\n",
    "##### 2018\n",
    "\n",
    "\n",
    "Area\n",
    "Item\n",
    "Year\n",
    "Value\n",
    "Unit\n",
    "Flag\n",
    "Flag Description\n",
    "Egypt\tWheat\t2018\t12522275.27\tt\t-\t-\n",
    "Indonesia\tWheat\t2018\t10096298.93\tt\t-\t-\n",
    "Algeria\tWheat\t2018\t8422056.96\tt\t-\t-\n",
    "Italy\tWheat\t2018\t7453326.38\tt\t-\t-\n",
    "Brazil\tWheat\t2018\t6817138.26\tt\t-\t-\n",
    "Philippines\tWheat\t2018\t6690772.39\tt\t-\t-\n",
    "Spain\tWheat\t2018\t6028087.66\tt\t-\t-\n",
    "Türkiye\tWheat\t2018\t5781711.54\tt\t-\t-\n",
    "Japan\tWheat\t2018\t5652192.67\tt\t-\t-\n",
    "Netherlands (Kingdom of the)\tWheat\t2018\t5566984.97\tt\t-\t-\n",
    "Viet Nam\tWheat\t2018\t5318644.73\tt\t-\t-\n",
    "Mexico\tWheat\t2018\t4920401.06\tt\t-\t-\n",
    "Bangladesh\tWheat\t2018\t4839308.1\tt\t-\t-\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2021\n",
    "element= \"Import Quantity\"\n",
    "product = \"Wheat\"\n",
    "\n",
    "wheat = pd_usecol[(pd_usecol[\"Year\"] == year) & (pd_usecol[\"Element\"] == \"Import Quantity\") & (pd_usecol[\"Item\"] == \"Wheat\") ]\n",
    "top_imports = wheat.groupby('Reporter Countries')['Value'].sum().reset_index()\n",
    "top_imports_sorted = top_imports.sort_values(by='Value', ascending=False).reset_index()\n",
    "\n",
    "# Value top \n",
    "wheat_val = pd_usecol[(pd_usecol[\"Year\"] == year) & (pd_usecol[\"Element\"] == \"Import Value\") & (pd_usecol[\"Item\"] == \"Wheat\") ]\n",
    "top_imports_val = wheat_val.groupby('Reporter Countries')['Value'].sum().reset_index()\n",
    "top_imports_sorted_val = top_imports_val.sort_values(by='Value', ascending=False).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_imports_sorted_val.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot quantity\n",
    "fig, axs = plt.subplots(1,2,figsize=(10,5))\n",
    "element= \"Import Quantity\"\n",
    "axs[0].bar(top_imports_sorted.loc[0:9,'Reporter Countries'],top_imports_sorted.loc[0:9,'Value']/1e6)\n",
    "axs[0].set_xticklabels(top_imports_sorted.loc[0:9,'Reporter Countries'],rotation=50,ha='right')\n",
    "axs[0].set_title(element +' ' + product + ' '+ str(year) + ' (tonnes)')\n",
    "axs[0].set_ylabel('Million Tonnes')\n",
    "\n",
    "#Plot value \n",
    "element= \"Import Value\"\n",
    "axs[1].bar(top_imports_sorted_val.loc[0:9,'Reporter Countries'],top_imports_sorted_val.loc[0:9,'Value'])\n",
    "axs[1].set_xticklabels(top_imports_sorted_val.loc[0:9,'Reporter Countries'],rotation=50)\n",
    "axs[1].set_title(element +' ' + product + ' '+ str(year) + ' (USD $)')\n",
    "axs[1].set_ylabel('1000 USD$')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non overlap: Sudan for imports in $. \n",
    "\n",
    "# Check if wheat for sudan in 2018 exists in OG data:\n",
    "wheat_og= pd_main[(pd_main[\"Year\"] == year) & (pd_main[\"Element\"] == element) & (pd_main[\"Item\"] == product) & (pd_main[\"Reporter Countries\"] == \"Sudan\")  ]\n",
    "\n",
    "\n",
    "wheat_usecol= pd_usecol[(pd_usecol[\"Year\"] == year) & (pd_usecol[\"Element\"] == element) & (pd_usecol[\"Item\"] == product) & (pd_usecol[\"Reporter Countries\"] == \"South Sudan\")  ]\n",
    "wheat_usecol\n",
    "# Sudan Not found in downloaded data \n",
    "# Iran Wheat 2021 not found in downloaded data \n",
    "wheat_usecol.Value.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_usedata.to_csv('../../Data/Trade_filtered.csv',encoding = 'utf-8') #save data \n",
    "pd_usedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Reconciliation\n",
    "**We have to take into account the following:**\n",
    "\n",
    "\"In the construction of trade matrices, one should consider that the same trade flow can be reported twice in the FAOSTAT database, once by the exporting country and once by the importing country. When a trade flow is reported by only one of the two countries, the reported flow is used to construct the matrix (single record); this is the case for 40 % of records in the database. All other records are “double” (reported twice) and require a comparison between the declarations of the exporting and the importing countries, which are usually different, with a mean (absolute) relative difference, across all goods, countries, and years, of 61%. The choice of a value from two double records is called “reconciliation”, and the method adopted here is based on the identification of the most reliable reporting country among the two involved in each flow, and the use of the flow being reported by it. The reliability of countries is measured per commodity and per year with a data-based approach detailed below and adapted from Gehlhar (1996).\" Source: https://doi.org/10.5194/essd-13-2025-2021 (they also provide guidance on country changes at the end of the supplementary material, see \"Associations\" at https://essd.copernicus.org/preprints/essd-2020-226/essd-2020-226-supplement.pdf)\n",
    "\n",
    "\n",
    "The team behind the paper above computes a country reliability formula and selects which source of information to keep for duplicated entries (when both countries report the flow)\n",
    "\n",
    "\n",
    "**This index is easier to apply in R, hence I move from here to R Studio.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
