{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering and aggregation\n",
    "- **1. Keep only food products:** Using the item list with the binary variable ('Is food').\n",
    "- **2. Aggregate products:** Use FoodEx classification to aggregate food products into Food categories to simplify the analysis. \n",
    "- **3. Explore data availability:** We want to see how many countries are trading per product and year (nodes network) and how many transactions (edges network) exist among them. \n",
    "- **4. Temporal aggregation** See if data availability is better after aggregating the data into 2-year intervals (36years/2= 18intervals). \n",
    "- **5. Filter non-relevant products:** Remove products with 0 transactions for more than half of the intervals. They don't have enough data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import networkx as nx\n",
    "import geopandas as gpd #pip installed\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib.pyplot as plt \n",
    "import ast\n",
    "import seaborn as sns\n",
    "import openpyxl\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION DEFINITION\n",
    "# working\n",
    "def Links_per_year(data_g,year_estimation,key):\n",
    "    trades_year= data_g.groupby('year').apply(lambda group: group.shape[0]) #num trades per year.\n",
    "    year_estimation.loc[key, trades_year.index]= trades_year    \n",
    "\n",
    "def Countries_trading_per_year(data_g,year_estimation,key):\n",
    "    trades_year= data_g.groupby('year').apply(lambda group: len(set(group.origin_country_ISO.unique()).\n",
    "                                              union(group.destin_country_ISO.unique()))) #num trades per year.\n",
    "    year_estimation.loc[key, trades_year.index]= trades_year    \n",
    "\n",
    "def Year_group_sum (data, year_split):\n",
    "    \"\"\"\n",
    "    Aggregate yearly data based on a specified list of year ranges.\n",
    "\n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): Input DataFrame containing columns 'year', 'item', 'unit', 'origin_country', 'destin_country', and 'value'.\n",
    "    - year_split (list): List of years to filter and aggregate.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with aggregated values for the specified year ranges.\n",
    "    \n",
    "    Example:\n",
    "    >>> input_data = pd.DataFrame({'year': [2010, 2011, 2011, 2012, 2013],\n",
    "                                   'item': ['A', 'B', 'A', 'B', 'A'],\n",
    "                                   'unit': ['KG', 'KG', 'L', 'L', 'KG'],\n",
    "                                   'origin_country': ['US', 'CA', 'US', 'CA', 'US'],\n",
    "                                   'destin_country': ['CA', 'US', 'CA', 'US', 'CA'],\n",
    "                                   'value': [10, 20, 30, 40, 50]})\n",
    "    >>> years_to_aggregate = [2011, 2012, 2013]\n",
    "    >>> result_df = Year_aggregation(input_data, years_to_aggregate)\n",
    "    \"\"\"\n",
    "    filt_data= data.loc[data.year.isin(year_split),:]\n",
    "    year_label = str(min(year_split))+'-'+str(max(year_split))\n",
    "\n",
    "    data_grouped= filt_data.groupby(['Food_group','unit','origin_country','destin_country'])\n",
    "    filt_data.loc[:,'value']= data_grouped['value'].transform('sum')\n",
    "    filt_data.loc[:,'year']= year_label\n",
    "    print(year_split)\n",
    "    return(filt_data)\n",
    "\n",
    "def Year_aggregation(data, Num_parts):\n",
    "    # 36 timepoints \n",
    "    year_range = np.arange(data.year.min(),data.year.max()+1)\n",
    "\n",
    "    split_years = np.array_split(year_range, Num_parts)\n",
    "\n",
    "    # Year aggregation\n",
    "    out_data = Year_group_sum(data,split_years[0])\n",
    "\n",
    "    for s in split_years[1:]: \n",
    "        out_data=pd.concat([out_data, Year_group_sum(data,s)], ignore_index=True)\n",
    "\n",
    "    data_agr = out_data.drop_duplicates()\n",
    "    return data_agr\n",
    "    \n",
    "def Clean_item_names(data, item_list,foodex_levels):\n",
    "    \"\"\"Clean `item` names from 'data' to match 'item_list'.\n",
    "\n",
    "    The function cleans special characters in item names and matches the items between 'data' and 'item_list'\n",
    "    to guarantee the consistancy between both dataframes. \n",
    "\n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): Input DataFrame containing an 'item' column to be cleaned and filtered.\n",
    "    - item_list (pd.DataFrame): DataFrame containing the metadata file for each item.\n",
    "\n",
    "    Returns:\n",
    "    Tuple: A tuple containing two DataFrames:\n",
    "        1. Cleaned and filtered DataFrame 'data' with item names matched to the 'item_list'.\n",
    "        2. Original 'item_list' DataFrame with cleaned item names.\n",
    "\n",
    "    Example:\n",
    "    >>> cleaned_data, cleaned_item_list = Clean_item_names(input_data, valid_items)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load item codes\n",
    "    replacements = {'�': 'é', ';': ',', '\"': ''}\n",
    "\n",
    "    item_list['item']=item_list.item.replace(replacements,regex=True)\n",
    "\n",
    "    data['item']=data.item.replace(replacements,regex=True)\n",
    "\n",
    "    # Match items\n",
    "    # print('Items in data, missing in item list: ',len(set(data.item.unique()).difference(set(item_list.item.unique())))) # Check only to confirm that we are not missing any product in item_list\n",
    "    # print('Items in item_list, not in data: ',set(item_list.item.unique()).difference(set(data.item.unique()))) # Check only to confirm that we are not missing any product in item_list\n",
    "    \n",
    "    # Add Foodex labels to item_list \n",
    "    item_list= pd.merge(item_list, foodex_levels,left_on='L1_foodex',right_on='Code',how='left').drop(columns='Code').rename(columns={'IndentedTree':'Food_group'})\n",
    "\n",
    "    # Merge itemlist & data:\n",
    "    data= pd.merge(data, item_list.loc[:,['item','Is_food','L1_foodex','Food_group']],on='item',how='left',copy=False)\n",
    "    print(data)\n",
    "    return data, item_list\n",
    "\n",
    "def Check_data_available(data):\n",
    "    # Define matrix\n",
    "    data_g = data.groupby(['Food_group','unit'])\n",
    "\n",
    "    dict_keys=data_g.groups.keys()\n",
    "    \n",
    "    if isinstance(data.year.iloc[0], str):\n",
    "        year_range =data.year.unique()\n",
    "    else: \n",
    "        year_range = np.arange(data.year.min(),data.year.max()+1)\n",
    "    \n",
    "    year_estimation= pd.DataFrame(np.zeros([len(dict_keys),len(year_range)]),columns=year_range,index=dict_keys)\n",
    "\n",
    "    # Define matrix\n",
    "    year_estimation= pd.DataFrame(np.zeros([len(dict_keys),len(year_range)]),columns=year_range,index=dict_keys)\n",
    "    year_estimation_links=year_estimation.copy()\n",
    "\n",
    "    # Fill matrices\n",
    "    for i in list(dict_keys):\n",
    "        Countries_trading_per_year(data_g.get_group(i),year_estimation, i)\n",
    "\n",
    "        Links_per_year(data_g.get_group(i),year_estimation_links, i)\n",
    "    return year_estimation, year_estimation_links, year_range\n",
    "\n",
    "def Plot_data_available(year_estimation,year_estimation_links,year_range,flag_save): \n",
    "    fig, ax = plt.subplots(1,2,figsize=(10,5))\n",
    "    flat_links= year_estimation_links.to_numpy().flatten()\n",
    "    ax[0].hist(flat_links,bins = 30, density=True)\n",
    "    ax[0].set_ylabel('Probability')\n",
    "    ax[0].set_xlabel('Average. num of transactions')\n",
    "    ax[0].set_title('Distribution of the av. num of transactions')\n",
    "\n",
    "    # Plot data availability for aggregates:\n",
    "    #year_est_bin= 1*(year_estimation>0)\n",
    "    '''\n",
    "    mean_size = 100*((year_est_bin).sum())/year_estimation.shape[0]\n",
    "    \n",
    "    ax[0].plot(year_range,mean_size)\n",
    "    ax[0].set_ylim(0,100)\n",
    "    ax[0].set_ylabel('Percentage of food_groups with data(%)')\n",
    "    ax[0].title.set_text('Food_grups with some trade data')\n",
    "    '''\n",
    "    # Mean number of links per year\n",
    "    mean_links = year_estimation.median(axis=0)\n",
    "\n",
    "    inf_perc = np.quantile(year_estimation, 0.025,axis=0)\n",
    "    sup_perc = np.quantile(year_estimation, 0.975,axis=0)\n",
    "            \n",
    "    std_links = year_estimation.std()\n",
    "\n",
    "    ax[1].plot(year_range, mean_links, color='blue', alpha=0.2, label='Standard Deviation')\n",
    "    ax[1].fill_between(year_range, inf_perc, sup_perc, color='blue', alpha=0.2, label='Standard Deviation')\n",
    "\n",
    "    #ax[1].set_xlim(min(year_range),max(year_range))\n",
    "    #ax[1].set_xticks(ax[0].get_xticks(),labels=ax[0].get_xticklabels(),rotation=40,ha='right')\n",
    "\n",
    "    #plt.ylabel('Average number of links per network (95% CI)')\n",
    "    ax[1].set_ylabel('Num. countries')\n",
    "    ax[1].title.set_text('Countries trading each food group (median + 95% CI)')\n",
    "    ax[1].set_xlabel('Year')\n",
    "    \n",
    "    # \n",
    "    fig,ax= plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    sns.heatmap(year_estimation_links, cmap=sns.color_palette(\"viridis\", as_cmap=True), fmt='g', cbar=True,ax=ax,norm=LogNorm(vmin=1))\n",
    "    ax.set_facecolor('gray')\n",
    "    ax.set_title ('Number of transactions per food group')\n",
    "    plt.show()\n",
    "    fig.savefig('prova_'+str(flag_save)+'.pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only Items that are considered food Items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num unique products, 558\n",
      "         destin_country_ISO origin_country_ISO  \\\n",
      "0                        AM                 UA   \n",
      "1                        AM                 UA   \n",
      "2                        AM                 UA   \n",
      "3                        AM                 UA   \n",
      "4                        AM                 UA   \n",
      "...                     ...                ...   \n",
      "32426007                 NP                 SN   \n",
      "32426008                 NP                 SN   \n",
      "32426009                 NP                 SN   \n",
      "32426010                 NP                 SN   \n",
      "32426011                 NP                 SN   \n",
      "\n",
      "                                                       item  year      unit  \\\n",
      "0         Anise, badian, coriander, cumin, caraway, fenn...  2003    tonnes   \n",
      "1         Anise, badian, coriander, cumin, caraway, fenn...  2019    tonnes   \n",
      "2         Anise, badian, coriander, cumin, caraway, fenn...  2003  1000 US$   \n",
      "3         Anise, badian, coriander, cumin, caraway, fenn...  2019  1000 US$   \n",
      "4                                               Apple juice  2003    tonnes   \n",
      "...                                                     ...   ...       ...   \n",
      "32426007                      Crude organic material n.e.c.  2020  1000 US$   \n",
      "32426008                                        Food wastes  2017    tonnes   \n",
      "32426009                                        Food wastes  2017  1000 US$   \n",
      "32426010                                        Other birds  2017    tonnes   \n",
      "32426011                                        Other birds  2017  1000 US$   \n",
      "\n",
      "          value origin_country destin_country  Is_food L1_foodex  \\\n",
      "0           0.0        Ukraine        Armenia        1     A011X   \n",
      "1           0.0        Ukraine        Armenia        1     A011X   \n",
      "2           0.0        Ukraine        Armenia        1     A011X   \n",
      "3           0.0        Ukraine        Armenia        1     A011X   \n",
      "4          18.0        Ukraine        Armenia        1     A039K   \n",
      "...         ...            ...            ...      ...       ...   \n",
      "32426007   10.0        Senegal          Nepal        0     A0BYQ   \n",
      "32426008    9.0        Senegal          Nepal        0     A0BRR   \n",
      "32426009    6.0        Senegal          Nepal        0     A0BRR   \n",
      "32426010    0.1        Senegal          Nepal        1     A01QR   \n",
      "32426011    1.0        Senegal          Nepal        1     A01QR   \n",
      "\n",
      "                                                 Food_group  \n",
      "0                        Legumes, nuts, oilseeds and spices  \n",
      "1                        Legumes, nuts, oilseeds and spices  \n",
      "2                        Legumes, nuts, oilseeds and spices  \n",
      "3                        Legumes, nuts, oilseeds and spices  \n",
      "4         Fruit and vegetable juices and nectars (includ...  \n",
      "...                                                     ...  \n",
      "32426007                                                NaN  \n",
      "32426008                                                NaN  \n",
      "32426009                                                NaN  \n",
      "32426010                             Meat and meat products  \n",
      "32426011                             Meat and meat products  \n",
      "\n",
      "[32426012 rows x 11 columns]\n",
      "Unique food products in data after filtering, 431\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_pickle('../Data/Trade_geo.pkl')\n",
    "#data=data.loc[data.value>0,:].reset_index()\n",
    "\n",
    "country_metadata = pd.read_pickle('../Data/Country_info.pkl')\n",
    "shape_file = pd.read_pickle('../Data/Shapefile_with_positions.pkl')\n",
    "print('Num unique products, '+ str(len(data.item.unique())))\n",
    "\n",
    "# Item list prep\n",
    "#item_list =pd.read_csv('../Data/raw_trade/Trade_ItemCodes_food_products.csv', sep=',', encoding='utf-8').rename(columns={'Item':'item'})\n",
    "\n",
    "item_list =pd.read_csv('../Data/raw_trade/Trade_ItemCodes_Isfood_foodex.csv', sep=',', encoding='utf-8').rename(columns={'Item':'item'})\n",
    "foodex_levels =pd.read_excel('../Data/Foodex_raw/Exposure_Hierarchy_revision2.xlsx').loc[:,['Code','IndentedTree']]\n",
    "\n",
    "data, item_list = Clean_item_names(data, item_list,foodex_levels)\n",
    "\n",
    "# Filter food items only: \n",
    "data = data.loc[data['Is_food']>0,:].drop(columns=['Is_food'])\n",
    "\n",
    "print('Unique food products in data after filtering, '+ str(len(data.item.unique())))\n",
    "\n",
    "# Filter items in foodex \n",
    "food_item_list = item_list.loc[item_list.item.isin(data.item.unique()),:]\n",
    "food_item_list\n",
    "\n",
    "# Save file \n",
    "data.to_pickle('../Data/Data_food_groups.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FoodEx aggregation  (OLD CODEEE)\n",
    "Aggregate data in the food groups defined by the FoodEx standard. (old code: NOW THIS IS ADDED IN THE 5.1 file, because we can choose the level of aggregation to study)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now moved to mulitlayer_year_map_working\n",
    "# Agregate data in food groups\n",
    "data = FoodEx_aggregation(data)\n",
    "data = data.sort_values(by=['Food_group','year'])\n",
    "# Save data after agregation and filtering\n",
    "data.reset_index().to_pickle('../Data/Data_food_groups_old.pkl') #<-maybe useful for a bit to compare and check I didn't mess up but the rest can be removed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data availability after product aggregation\n",
    "\n",
    "We see a very skewed distribution on data availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_no = data.loc[(data.Food_group != 'Other ingredients') & (data.unit == '1000 US$'),:]\n",
    "\n",
    "data_c = data.loc[(data.unit == '1000 US$'),:]\n",
    "\n",
    "avail_countries, avail_trade, year_range = Check_data_available(data_c)\n",
    "\n",
    "Plot_data_available(avail_countries,avail_trade,year_range,'og')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other ingredients has almost no data associated. Therefore, we will remove it (for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no = data.loc[(data.Food_group != 'Other ingredients') & (data.unit == '1000 US$'),:]\n",
    "\n",
    "avail_countries_no, avail_trade_no, year_range_no = Check_data_available(data_no)\n",
    "\n",
    "Plot_data_available(avail_countries_no,avail_trade_no,year_range_no,'no_other')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Year aggregation\n",
    "Agregate data to reduce the number of timepoints and get a better resolution in years with low data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Num_parts= 18\n",
    "data_agr = Year_aggregation(data_no,Num_parts)\n",
    "\n",
    "# Check data quality\n",
    "avail_countries_agr, avail_trade_agr, year_range_agr = Check_data_available(data_agr)\n",
    "\n",
    "Plot_data_available(avail_countries_agr,avail_trade_agr,year_range_agr,'year')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data_agr.year[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD CODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avail_countries_agr, avail_trade_agr, year_range_agr = Check_data_available(data_agr)\n",
    "\n",
    "Plot_data_available(avail_countries_agr,avail_trade_agr,year_range_agr)\n",
    "\n",
    "# Define matrix\n",
    "agr_range = data_agr.year.unique()\n",
    "data_agr_g = data_agr.groupby(['item','unit'])\n",
    "\n",
    "dict_keys=data_agr_g.groups.keys()\n",
    "year_est_ag= pd.DataFrame(np.zeros([len(dict_keys),len(agr_range)]),columns=agr_range,index=dict_keys)\n",
    "year_est_ag_l=year_est_ag.copy()\n",
    "\n",
    "# Fill matrices\n",
    "for i in list(dict_keys):\n",
    "    Countries_trading_per_year(data_agr_g.get_group(i),year_est_ag, i)\n",
    "\n",
    "    Links_per_year(data_agr_g.get_group(i), year_est_ag_l, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat plots after filtering: \n",
    "year_est_bin= 1*(year_est_ag>0)\n",
    "mean_size = 100*((year_est_bin).sum())/year_est_ag.shape[0]\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(10,5))\n",
    "ax[0].plot(agr_range,mean_size)\n",
    "ax[0].set_ylim(0,100)\n",
    "ax[0].set_xticks(ax[0].get_xticks(),labels=ax[0].get_xticklabels(),rotation=40,ha='right')\n",
    "ax[0].set_ylabel('Percentage of all products with data(%)')\n",
    "ax[0].title.set_text('Total products with some trade data')\n",
    "\n",
    "# Mean number of links per year\n",
    "mean_links = year_est_ag.median(axis=0)\n",
    "\n",
    "inf_perc = np.quantile(year_est_ag, 0.025,axis=0)\n",
    "sup_perc = np.quantile(year_est_ag, 0.975,axis=0)\n",
    "        \n",
    "std_links = year_est_ag.std()\n",
    "\n",
    "ax[1].plot(agr_range, mean_links, color='blue', alpha=0.2, label='Standard Deviation')\n",
    "ax[1].fill_between(agr_range, inf_perc, sup_perc, color='blue', alpha=0.2, label='Standard Deviation')\n",
    "ax[1].set_xticks(ax[1].get_xticks(),labels=ax[1].get_xticklabels(),rotation=40,ha='right')\n",
    "\n",
    "ax[1].set_ylabel('Num. countries')\n",
    "ax[1].title.set_text('Median number of countries per product (95% CI)')\n",
    "ax[1].set_xlabel('Year intervals')\n",
    "\n",
    "\n",
    "fig,axes=plt.subplots(2,1,figsize=(100,20))\n",
    "sns.heatmap(year_estimation.T, cmap='coolwarm',cbar=True,ax=axes[0],xticklabels=True,vmax=200,vmin=0)\n",
    "sns.heatmap(year_est_ag.T, cmap='coolwarm',cbar=True,ax=axes[1],vmax=200,vmin=0)\n",
    "#fig.savefig('prova.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop series without data for more than half of the years(even after aggregation)\n",
    "If a product has more than half of the years without transactions:\n",
    "\n",
    "- **Option 1:** Not enough data to study it. *(Remove it)*\n",
    "- **Option 2:** Super small and localised product that it is not interesting for global trade. *(Remove it)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_agr_g = data_agr.groupby(['item','unit'])\n",
    "# Fill matrices\n",
    "zero_links=(year_est_ag_l==0).sum(axis =1)\n",
    "to_drop= zero_links.loc[zero_links>=len(agr_range)/2,:]\n",
    "\n",
    "key_to_drop=list(to_drop.keys())\n",
    "key_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter gropus with not enough data:\n",
    "data_filt= data_agr.copy()\n",
    "for key in key_to_drop:\n",
    "    bool_condit= (data_filt.item == key[0]) & (data_filt.unit == key[1])\n",
    "    #data_agr.drop(data_agr_g2.get_group(key).index, inplace=True)\n",
    "    data_filt= data_filt.loc[~bool_condit,:]\n",
    "\n",
    "# Sanity check\n",
    "print('Before filtering low data:',len(data_agr.loc[:,['item','unit']].drop_duplicates()))\n",
    "print('Filtered:',len(data_filt.loc[:,['item','unit']].drop_duplicates()))\n",
    "print('To remove:',len(key_to_drop))\n",
    "print('Total products with data:',len(data_filt.item.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat data availability after filtering: \n",
    "\n",
    "# Define matrix\n",
    "data_filt_g = data_filt.groupby(['item','unit'])\n",
    "\n",
    "dict_keys=data_filt_g.groups.keys()\n",
    "year_est_ag= pd.DataFrame(np.zeros([len(dict_keys),len(agr_range)]),columns=agr_range,index=dict_keys)\n",
    "year_est_ag_l=year_est_ag.copy()\n",
    "\n",
    "# Fill matrices\n",
    "for i in list(dict_keys):\n",
    "    Countries_trading_per_year(data_filt_g.get_group(i),year_est_ag, i)\n",
    "\n",
    "    Links_per_year(data_filt_g.get_group(i), year_est_ag_l, i)\n",
    "\n",
    "\n",
    "# Plot data availability for aggregates:\n",
    "year_est_bin= 1*(year_est_ag>0)\n",
    "mean_size = 100*((year_est_bin).sum())/year_est_ag.shape[0]\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(10,5))\n",
    "ax[0].plot(agr_range,mean_size)\n",
    "ax[0].set_ylim(0,100)\n",
    "ax[0].set_xticks(ax[0].get_xticks(),labels=ax[0].get_xticklabels(),rotation=40,ha='right')\n",
    "ax[0].set_ylabel('Percentage of all products with data(%)')\n",
    "ax[0].title.set_text('Total products with some trade data')\n",
    "\n",
    "# Mean number of links per year\n",
    "mean_links = year_est_ag.median(axis=0)\n",
    "\n",
    "inf_perc = np.quantile(year_est_ag, 0.025,axis=0)\n",
    "sup_perc = np.quantile(year_est_ag, 0.975,axis=0)\n",
    "        \n",
    "std_links = year_est_ag.std()\n",
    "\n",
    "ax[1].plot(agr_range, mean_links, color='blue', alpha=0.2, label='Standard Deviation')\n",
    "ax[1].fill_between(agr_range, inf_perc, sup_perc, color='blue', alpha=0.2, label='Standard Deviation')\n",
    "ax[1].set_xticks(ax[1].get_xticks(),labels=ax[1].get_xticklabels(),rotation=40,ha='right')\n",
    "\n",
    "#plt.ylabel('Average number of links per network (95% CI)')\n",
    "ax[1].set_ylabel('Num. countries')\n",
    "ax[1].title.set_text('Median number of countries per product (95% CI)')\n",
    "ax[1].set_xlabel('Year intervals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare filtered samples from original estimation: \n",
    "filt_compare = year_estimation.loc[year_est_ag.index,:]\n",
    "\n",
    "fig,axes=plt.subplots(2,1,figsize=(100,20))\n",
    "sns.heatmap(filt_compare.T, cmap='coolwarm',cbar=True,ax=axes[0],xticklabels=False, vmax=200,vmin=0)\n",
    "sns.heatmap(year_est_ag.T, cmap='coolwarm',cbar=True,ax=axes[1],vmax=200,vmin=0)\n",
    "#fig.savefig('prova.pdf')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
